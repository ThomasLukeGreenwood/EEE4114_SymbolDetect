{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jana\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining all my functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aspect ratio: 2.0\n"
     ]
    }
   ],
   "source": [
    "def calculate_aspect_ratio(width, height):\n",
    "    \"\"\"\n",
    "    Calculate the aspect ratio of an object given its width and height.\n",
    "    \n",
    "    Args:\n",
    "        width (float or int): Width of the object.\n",
    "        height (float or int): Height of the object.\n",
    "        \n",
    "    Returns:\n",
    "        float: Aspect ratio of the object (width / height).\n",
    "    \"\"\"\n",
    "    return width / height if height != 0 else 0.0\n",
    "\n",
    "# Example usage:\n",
    "width = 100  # Example width of the object\n",
    "height = 50  # Example height of the object\n",
    "\n",
    "aspect_ratio = calculate_aspect_ratio(width, height)\n",
    "print(\"Aspect ratio:\", aspect_ratio)\n",
    "\n",
    "#calculate gradients\n",
    "def gradients(all_images): #array of image vectors as input\n",
    "    gradientsArray = []\n",
    "    \n",
    "    for image in all_images:\n",
    "        cv2.imwrite('myImage.png', image)\n",
    "        img = cv2.imread('myImage.png', cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        #obtain x and y gradients\n",
    "        sobel_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)\n",
    "        sobel_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "        #calculate magnitude and direction\n",
    "        gradient_magnitude = np.sqrt(sobel_x**2 + sobel_y**2)\n",
    "        gradient_direction = np.arctan2(sobel_y, sobel_x)\n",
    "        gradientsArray.append([gradient_magnitude, gradient_direction])\n",
    "        \n",
    "    return np.array(gradientsArray)\n",
    "\n",
    "#calculate horizontal and vertical profiles of each image\n",
    "def profiles(all_images): #array of image vectors as input\n",
    "    profilesArray = []\n",
    "    for image in all_images:\n",
    "       \n",
    "        #calculate horizontal profiles\n",
    "        horizontal_profile = np.sum(image, axis=1)\n",
    "        horizontal_profile = horizontal_profile/np.max(horizontal_profile)\n",
    "        \n",
    "        #calculate vertical profiles\n",
    "        vertical_profile = np.sum(image, axis=0)\n",
    "        vertical_profile = vertical_profile/np.max(vertical_profile)\n",
    "        \n",
    "        profilesArray.append([horizontal_profile, vertical_profile])\n",
    "    \n",
    "    return np.array(profilesArray)\n",
    "\n",
    "def calculate_stats(all_images): #takes in array of vector images\n",
    "    statsArray = []\n",
    "    for image in all_images:\n",
    "        # Calculate mean\n",
    "        mean = np.mean(image)\n",
    "\n",
    "        # Calculate standard deviation\n",
    "        std_dev = np.std(image)\n",
    "\n",
    "        # Calculate skewness\n",
    "        skewness = skew(image.reshape(-1))  # Reshape to 1D array for skew function\n",
    "\n",
    "        # Calculate kurtosis\n",
    "        kurt = kurtosis(image.reshape(-1))  # Reshape to 1D array for kurtosis function\n",
    "        \n",
    "        statsArray.append([mean, std_dev, skewness, kurt])\n",
    "    return np.array(statsArray)\n",
    "\n",
    "def calculate_hu_moments(all_images): #takes in vector\n",
    "    momentsArray = []\n",
    "    for image_array in all_images:\n",
    "        cv2.imwrite('myImage.png', image_array) #convert vector to image\n",
    "        image = cv2.imread('myImage.png', cv2.IMREAD_GRAYSCALE) #read in image as grayscale (needs to be grayscale for moments function)\n",
    "        _,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY) \n",
    "        \n",
    "        #calculate moments\n",
    "        moments = cv2.moments(image) \n",
    "        \n",
    "        # Calculate Hu Moments\n",
    "        huMoments = cv2.HuMoments(moments)\n",
    "        momentsArray.append(huMoments)\n",
    "        \n",
    "    return np.array(momentsArray)\n",
    "\n",
    "#outputs 2D vector (one 1D array dedicated to each image)\n",
    "def combine_inputs(all_images, s = 1, h = 1, p = 1, g=1): #combine_inputs(vector array, includeMoments = 1, includeHuMoments = 1, includeProfiles = 1)\n",
    "    \n",
    "    #grayscale and flatten image array\n",
    "    all_images_flat = tf.keras.layers.Flatten()(all_images)\n",
    "    \n",
    "    #moments\n",
    "    if s == 1:\n",
    "        stats_array = calculate_stats(all_images) #np array of stats for each image\n",
    "    \n",
    "    #hu moments\n",
    "    if h == 1:\n",
    "        hu_moments_array = calculate_hu_moments(all_images) #np array of hu moments for each image\n",
    "        #flatten hu moments array\n",
    "        hu_moments_flat = tf.keras.layers.Flatten()(hu_moments_array)\n",
    "    \n",
    "    #profiles\n",
    "    if p == 1:\n",
    "        profiles_array = profiles(all_images)\n",
    "        profiles_flat = tf.keras.layers.Flatten()(profiles_array)\n",
    "    \n",
    "    output = all_images_flat\n",
    "    \n",
    "    #gradients\n",
    "    if g == 1:\n",
    "        gradients_array = gradients(all_images)\n",
    "        gradients_flat = tf.keras.layers.Flatten()(gradients_array)\n",
    "    \n",
    "    #output correct vector\n",
    "    if s == 1:\n",
    "        output = np.concatenate((output, stats_array), axis = -1)\n",
    "    if h == 1:\n",
    "        output = np.concatenate((output, hu_moments_flat), axis = -1)\n",
    "    if p == 1:\n",
    "        output = np.concatenate((output, profiles_flat), axis = -1)\n",
    "    if g == 1:\n",
    "        output = np.concatenate((output, gradients_flat), axis = -1)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initial development purposes, I'm using the mnist dataset of handdrawn digits.\n",
    "This should be removed for actual project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist #collect data\n",
    "\n",
    "#split data into training and testing data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining inputs with mnist data\n",
    "input_train = combine_inputs(x_train, 0, 0, 0, 1)\n",
    "input_test = combine_inputs(x_test, 0, 0, 0, 1)\n",
    "\n",
    "#normalise data\n",
    "input_train = tf.keras.utils.normalize(input_train, axis = 1)\n",
    "input_test = tf.keras.utils.normalize(input_test, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For initial development purposes, this is the NN I've been using. Not at all optimised for this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7996 - loss: 0.6270\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9510 - loss: 0.1602\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.9653 - loss: 0.1097\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9728 - loss: 0.0791\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.9799 - loss: 0.0636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2c64be72bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "#model.add(tf.keras.layers.Flatten(input_shape=(28, 28))) - only needs to be used if just using mnist data and not extra info\n",
    "model.add(tf.keras.layers.Dense(128, activation = 'relu'))  #add dense layer, where each neuron is connected to each neuron of next layer\n",
    "model.add(tf.keras.layers.Dense(128, activation = 'relu')) \n",
    "model.add(tf.keras.layers.Dense(128, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='softmax')) #each unit represents number (0-9); softmax ensures all outputs add up to 1 (essentially outputs are percentages)\n",
    "                                 \n",
    "#compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#train the model\n",
    "model.fit(input_train, y_train, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(input_test, y_test)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(input_test, y_test)\n",
    "\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated orientation: nan\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
